{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e04f0b",
   "metadata": {},
   "source": [
    "# Calibration and uncertainty propagation\n",
    "Up until now we have been creating models that may accurately represent the local epidemic,\n",
    "but (at best) only provide one possible epidemic profile \n",
    "that would be consistent with our empiric observations of the epidemic. \n",
    "In this notebook, we extend this to obtain a range of parameter values and epidemic trajectories \n",
    "that would be consistent with the local observations, \n",
    "and thereby quantify the uncertainty in our simulations.\n",
    "\n",
    "Here we will learn how to use a Markov Chain Monte Carlo (MCMC) algorithm \n",
    "to calibrate an SIR model to epidemic data.\n",
    "That is, we will use a Bayesian sampling approach to estimate model parameters \n",
    "and to project the epidemic with uncertainty.\n",
    "Specifically, we will implement the Metropolis algorithm which is one type of MCMC.\n",
    "\n",
    "The field of Bayesian inference is large and well-established and includes many \n",
    "sophisticated algorithms which are outside the scope of this series.\n",
    "In this notebook, we'll just work through one of the most classical algorithms \n",
    "and encourage the reader to explore others \n",
    "through the copious resources available for learning \n",
    "Bayesian inference more generally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00c748-fc4b-45b7-a914-6460032b8733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Google Colab, run the following line of code to install the summer package\n",
    "# %pip install summerepi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19523e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "from summer2 import CompartmentalModel\n",
    "from summer2.parameters import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5f058",
   "metadata": {},
   "source": [
    "## Calibration data\n",
    "First, we'll create some dummy data we want our model to fit to.\n",
    "We'll use data points that look loosely like a bell,\n",
    "and so could plausibly represent an observed infectious disease epidemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296fb625",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"active_cases\":\n",
    "{\n",
    "    60.0: 3000.0,\n",
    "    80.0: 8500.0,\n",
    "    100.0: 21000.0,\n",
    "    120.0: 40000.0,\n",
    "    140.0: 44000.0,\n",
    "    160.0: 30000.0,\n",
    "    180.0: 16000.0,\n",
    "    200.0: 7000.0,\n",
    "}}\n",
    ")\n",
    "data[\"active_cases\"].plot(kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90cd4d",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We also need to define our mechanistic model \n",
    "of infectious disease transmission,\n",
    "and we'll just use the same one as we've been using throughout this series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03805af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sir_model(\n",
    "    config: dict,\n",
    ") -> CompartmentalModel:\n",
    "\n",
    "    model = CompartmentalModel(\n",
    "        times=(0.0, config[\"end_time\"]),\n",
    "        compartments=(\n",
    "            \"susceptible\", \n",
    "            \"infectious\", \n",
    "            \"recovered\",\n",
    "        ),\n",
    "        infectious_compartments=(\"infectious\",),\n",
    "    )\n",
    "    model.set_initial_population(\n",
    "        distribution=\n",
    "        {\n",
    "            \"susceptible\": config[\"population\"] - config[\"seed\"], \n",
    "            \"infectious\": config[\"seed\"],\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    model.add_infection_frequency_flow(\n",
    "        name=\"infection\", \n",
    "        contact_rate=Parameter(\"contact_rate\"), \n",
    "        source=\"susceptible\", \n",
    "        dest=\"infectious\",\n",
    "    )\n",
    "    model.add_transition_flow(\n",
    "        name=\"recovery\",\n",
    "        fractional_rate=Parameter(\"recovery\"),\n",
    "        source=\"infectious\",\n",
    "        dest=\"recovered\",\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0fa5be",
   "metadata": {},
   "source": [
    "## Trial run\n",
    "Next, let's just quickly run the model and get a sense \n",
    "for how the outputs look with some fairly arbitrary starting parameters.\n",
    "We'll get into adjusting these parameters later in the notebook.\n",
    "The initial fit is poor,\n",
    "so it should be pretty obvious if our algorithm is improving on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097609b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"population\": 1e6,\n",
    "    \"seed\": 100.0,\n",
    "    \"end_time\": 365.0,\n",
    "}\n",
    "parameters = {\n",
    "    \"contact_rate\": 0.3,\n",
    "    \"recovery\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sir_model = get_sir_model(model_config)\n",
    "sir_model.run(parameters)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"modelled\": sir_model.get_outputs_df()[\"infectious\"],\n",
    "        \"observed\": data[\"active_cases\"],\n",
    "    }\n",
    ").plot(kind=\"scatter\", labels={\"index\": \"time\", \"value\": \"number infectious\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5851e9",
   "metadata": {},
   "source": [
    "# Calibration\n",
    "Now we come to calibration, \n",
    "for which we'll need to define a few quantities \n",
    "and run through a bit of series to get started.\n",
    "\n",
    "## Posterior distribution\n",
    "The main objective of our calibration is to estimate the **_posterior distribution_** of the calibrated parameters. This is the probability distribution of the parameters that are able to describe our observations, given some prior knowledge about these parameters and a mathematical model. In other words, this tells us \"What values the parameters should take such that our model is able to capture the data, and given any prior information we had about the parameters before even running the model\".\n",
    "\n",
    "The following graph gives an overview of the posterior computation:\n",
    "![title](../images/calibration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49006091",
   "metadata": {},
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "We'll use Bayes' Theorem to decide parameter acceptance.\n",
    "The posterior probability of a parameter set $\\theta$ associated with the data $y$ is denoted $P(\\theta | y)$.\n",
    "\n",
    "Let's write the Bayes' Therorem for reference:\n",
    "$$P(\\theta | y) = \\frac{P(y | \\theta) \\times P(\\theta)}{P(y)} \\quad.$$\n",
    "\n",
    "Within the MCMC loop, we are principally interested in the \"acceptance ratio\" that defines the probability of acceptance of a newly proposed parameter set $\\theta '$, when the last accepted parameter set was $\\theta$. This is the ratio of the posterior probabilities between $\\theta '$ and $\\theta$:\n",
    "$$H := \\frac{P(\\theta ' | y)}{P(\\theta | y)} = \\frac{\\frac{P(y | \\theta ') \\times P(\\theta ')}{P(y)}}{\\frac{P(y | \\theta) \\times P(\\theta)}{P(y)}} = \\frac{P(y | \\theta ') \\times P(\\theta ')}{P(y | \\theta) \\times P(\\theta)} \\quad .$$\n",
    "\n",
    "If $H \\geq 1$, we'll automatically accept the proposed parameter set $\\theta'$. \n",
    "Otherwise, the proposed parameter set $\\theta'$ will be accepted with probability $H$. \n",
    "\n",
    "Here we will define the fundamental aspects of our MCMC calibration:\n",
    "- Our prior knowledge about the calibrated parameters: $P(\\theta)$\n",
    "- The likelihood associated with our model. This is the probability of observing the data under a given model parameterisation: $P(y|\\theta)$\n",
    "\n",
    "... and some other technical aspects:\n",
    "- Intitial point from which the MCMC algorithm starts\n",
    "- The proposal function (or jumping process), defining how we move around in our parameter space. This is defined by $\\pi(\\theta' | \\theta)$ which is the probability of reaching the parameter set $\\theta'$, when starting from the parameter set $\\theta$.\n",
    "\n",
    "### Log-transformed quantities\n",
    "The prior and likelihood quantities are often extremely small numbers in practice, \n",
    "which may make computation difficult due to computer precision limits. \n",
    "To avoid issues related to rounding, \n",
    "the probabilities are usually transformed \n",
    "using the logarithm function before calculation of the acceptance ratio.\n",
    "\n",
    "$$H=\\frac{P(y | \\theta ') \\times P(\\theta ')}{P(y | \\theta) \\times P(\\theta)} \\\\\n",
    "= exp\\Bigl( \\Bigl[\\ln\\bigl(P(y|\\theta')\\bigr) + \\ln\\bigl(P(\\theta')\\bigr) \\Bigr] - \\Bigl[\\ln\\bigl(P(y|\\theta)\\bigr) + \\ln\\bigl(P(\\theta)\\bigr) \\Bigr] \\Bigr) \\\\\n",
    "= exp( A(\\theta') - A(\\theta)) \\quad,\n",
    "$$\n",
    "where $A(\\theta):=\\ln\\bigl(P(y|\\theta)\\bigr) + \\ln\\bigl(P(\\theta)\\bigr) $ will be referred to as the acceptance quantity associated with parameter set $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f4cef",
   "metadata": {},
   "source": [
    "### Prior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a888af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_log_priors(\n",
    "    proposed_parameters: dict,\n",
    ") -> float:\n",
    "    \n",
    "    # Initialise the prior to 1\n",
    "    prior_log_proba = 0.0\n",
    "\n",
    "    # Use a uniform prior on [0., 0.5] for the contact_rate \n",
    "    prior_log_proba += stats.uniform.logpdf(x=proposed_parameters[\"contact_rate\"], loc=0.0, scale=0.5)\n",
    "\n",
    "    # Use a normal prior for the infection duration, with mode ten days\n",
    "    prior_log_proba += stats.norm.logpdf(x=proposed_parameters[\"recovery\"], loc=0.1, scale=0.1)\n",
    "\n",
    "    return prior_log_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2eec3",
   "metadata": {},
   "source": [
    "### Likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159cc81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_log_likelihood(\n",
    "    model: CompartmentalModel, \n",
    "    proposed_parameters: dict,\n",
    ") -> float:\n",
    "\n",
    "    # Build and run the model with the selected parameters\n",
    "    parameter_set = dict(proposed_parameters)\n",
    "    model.run(parameter_set)\n",
    "    modelled_active = model.get_outputs_df()[\"infectious\"]\n",
    "\n",
    "    # Calculate the log-likelihood associated with the model run\n",
    "    log_likelihood = 0.0\n",
    "    for data_time, data_value in data[\"active_cases\"].iteritems():\n",
    "        modelled_value = modelled_active.loc[data_time]\n",
    "        \n",
    "        # Use a normal likelihood with sd=100, centered on the model estimate\n",
    "        log_likelihood += stats.norm.logpdf(x=data_value, loc=modelled_value, scale=1e4)\n",
    "\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259d5e78",
   "metadata": {},
   "source": [
    "### Acceptance quantity\n",
    "As introduced above, \n",
    "this is calculated as the sum of the log prior and the log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ec261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acceptance_quantity(\n",
    "    model: CompartmentalModel, \n",
    "    proposed_parameters: dict,\n",
    ") -> float:\n",
    "\n",
    "    log_prior = evaluate_log_priors(proposed_parameters)\n",
    "    log_likelihood = evaluate_log_likelihood(model, proposed_parameters)\n",
    "    return log_prior + log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddc55d",
   "metadata": {},
   "source": [
    "### Proposal (jumping) function\n",
    "Get the next parameter set ($\\theta'$) from the previous one ($\\theta$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b340eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_parameter_set(\n",
    "    previous_parameters: dict, \n",
    "    jumping_sds: dict,\n",
    ") -> dict:\n",
    "    \n",
    "    proposed_parameters = {}\n",
    "    for param_name in [\"contact_rate\", \"recovery\"]:\n",
    "        proposed_parameters[param_name] = stats.norm.rvs(\n",
    "            loc=previous_parameters[param_name], \n",
    "            scale=jumping_sds[param_name]\n",
    "        )\n",
    "\n",
    "    return proposed_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d6517",
   "metadata": {},
   "source": [
    "## The Metropolis algorithm\n",
    "Now that we have all the pieces, \n",
    "we can put them all together in one loop to create the full algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a8715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_metropolis(\n",
    "    model: CompartmentalModel, \n",
    "    n_iter: int, \n",
    "    initial_parameters: dict, \n",
    "    jumping_sds:dict,\n",
    ") -> dict:\n",
    "   \n",
    "    mcmc_record = pd.DataFrame(\n",
    "        index=range(n_iter), \n",
    "        columns=list(parameters.keys()) + [\"acceptance_quantity\", \"changed_position\"],\n",
    "        dtype=float\n",
    "    )\n",
    "\n",
    "    current_parameters = initial_parameters\n",
    "    current_acceptance_quantity = evaluate_acceptance_quantity(model, current_parameters) \n",
    "\n",
    "    for i_run in range(n_iter):\n",
    "        \n",
    "        # Print a periodic update so we know things are running...\n",
    "        if (i_run % 1000) == 0:\n",
    "            print(f\"Running iter {i_run} of {n_iter} iterations\")\n",
    "        \n",
    "        # Propose a new parameter set and evaluate its acceptance quantity\n",
    "        proposed_parameters = propose_parameter_set(current_parameters, jumping_sds)\n",
    "        proposed_acceptance_quantity = evaluate_acceptance_quantity(model, proposed_parameters) \n",
    "\n",
    "        # Decide whether to accept the proposed parameters or not\n",
    "        if proposed_acceptance_quantity >= current_acceptance_quantity:\n",
    "            accept = 1\n",
    "        else:\n",
    "            accept_proba = np.exp(proposed_acceptance_quantity - current_acceptance_quantity)\n",
    "            accept = stats.binom.rvs(1, accept_proba)  # Flip a coin\n",
    "        \n",
    "        # Update the MCMC sampler in case of acceptance\n",
    "        if accept == 1:\n",
    "            current_parameters = proposed_parameters\n",
    "            current_acceptance_quantity = proposed_acceptance_quantity\n",
    "\n",
    "        # Record the current state\n",
    "        mcmc_record.loc[i_run] = {\n",
    "            \"contact_rate\": current_parameters[\"contact_rate\"], \n",
    "            \"recovery\": current_parameters[\"recovery\"], \n",
    "            \"acceptance_quantity\": float(current_acceptance_quantity),\n",
    "            \"changed_position\": accept\n",
    "        }\n",
    "    \n",
    "    return mcmc_record "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a90f7a",
   "metadata": {},
   "source": [
    "# Let's calibrate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2d4c9",
   "metadata": {},
   "source": [
    "## Run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b79a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 5000\n",
    "initial_parameters = {\n",
    "    \"contact_rate\": 0.15,\n",
    "    \"recovery\": 0.1,\n",
    "}\n",
    "jumping_sds = {\n",
    "    \"contact_rate\": 0.01,\n",
    "    \"recovery\": 0.01,\n",
    "}\n",
    "\n",
    "mcmc_output = sample_with_metropolis(sir_model, n_iterations, initial_parameters, jumping_sds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e8992",
   "metadata": {},
   "source": [
    "## Explore the MCMC outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f611dad",
   "metadata": {},
   "source": [
    "### Acceptance rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_accepted = mcmc_output[\"changed_position\"].sum()\n",
    "acceptance_perc = 100.0 * n_accepted / n_iterations\n",
    "print(f\"Our MCMC's acceptance rate is: {round(acceptance_perc,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf115b97",
   "metadata": {},
   "source": [
    "### Progression of the acceptance quantity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aebaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_output[\"acceptance_quantity\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4848601",
   "metadata": {},
   "source": [
    "### Parameter traces\n",
    "With our output dataframe,\n",
    "we can easily see the progression of the parameter\n",
    "values over successive MCMC iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_output[\"contact_rate\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe3222",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_output[\"recovery\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d38c06",
   "metadata": {},
   "source": [
    "### Burn-in\n",
    "We want to discard the parameter sets sampled before convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_in = round(n_iterations / 2.0)\n",
    "post_burn_in_mcmc_output = mcmc_output[burn_in:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94028318",
   "metadata": {},
   "source": [
    "### Posterior distributions\n",
    "With our output dataframes,\n",
    "we can easily inspect the distribution of the parameter samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_burn_in_mcmc_output[\"contact_rate\"].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f88941",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_burn_in_mcmc_output[\"recovery\"].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df80940",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_burn_in_mcmc_output.plot.scatter(x=\"contact_rate\", y=\"recovery\", color=\"acceptance_quantity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e1a86",
   "metadata": {},
   "source": [
    "### The best parameter set\n",
    "That is, the parameter set with the highest posterior value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_id = post_burn_in_mcmc_output[\"acceptance_quantity\"].idxmax()\n",
    "best_parameters = {\n",
    "    \"contact_rate\": post_burn_in_mcmc_output.loc[best_run_id][\"contact_rate\"],\n",
    "    \"recovery\": post_burn_in_mcmc_output.loc[best_run_id][\"recovery\"],\n",
    "}\n",
    "sir_model.run(best_parameters)\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"modelled\": sir_model.get_outputs_df()[\"infectious\"],\n",
    "        \"observed\": data[\"active_cases\"],\n",
    "    }\n",
    ").plot(kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960197b",
   "metadata": {},
   "source": [
    "### Plot sample of model runs\n",
    "Select 100 accepted parameter sets at random,\n",
    "run the model with these values \n",
    "and plot these against the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = min(100, n_iterations - burn_in)\n",
    "sampled_df = post_burn_in_mcmc_output.sample(n_samples)\n",
    "\n",
    "sampled_output_df = pd.DataFrame(index=sir_model.get_outputs_df().index)\n",
    "for i_run in sampled_df.index:\n",
    "    selected_parameters = {\n",
    "        \"contact_rate\": post_burn_in_mcmc_output.loc[i_run][\"contact_rate\"],\n",
    "        \"recovery\": post_burn_in_mcmc_output.loc[i_run][\"recovery\"],\n",
    "    }\n",
    "    sir_model.run(selected_parameters)\n",
    "    sampled_output_df[i_run] = sir_model.get_outputs_df()[\"infectious\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7610a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"matplotlib\"\n",
    "\n",
    "fig = sampled_output_df.plot(legend=None, figsize=(15, 8))\n",
    "fig.plot(data.index, data[\"active_cases\"], marker=\".\", lw=0, ms=10, color=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff20bf",
   "metadata": {},
   "source": [
    "## Some further considerations\n",
    "We have presented a very simple implementation of an MCMC-based calibration.\n",
    "However, in practice, there are quite a few other practical considerations\n",
    "when undertaking a Bayesian analysis. These include:\n",
    "\n",
    "- **Use of other MCMC algorithms.** Here we have implemented a \"simple\" Metropolis-Hastings algorithm, which is the simplest version of MCMCs.\n",
    "However, there exist other types of MCMCs including Gibbs sampling and the Hamiltonian Monte-Carlo. \n",
    "There are also other Bayesian sampling methods that don't verify the Markov property (i.e. not MCMC) \n",
    "but can be used for the same purpose. \n",
    "This includes adaptive Metropolis samplers such as the Haario algorithm.\n",
    "\n",
    "- **Use of multiple MCMC chains.** We have only implemented a single MCMC chain that explores the parameter set and samples posterior estimates. \n",
    "In practice, it is common to use multiple chains \n",
    "that can be run in parallel to generate more samples in the same period of time. \n",
    "Samples from the different chains are then combined \n",
    "and we can perform statistical tests to check for convergence \n",
    "and consistency between the chains (e.g. R-hat statistic).\n",
    "\n",
    "- **Non-symmetric proposal function.** We have used a symmetric proposal (jumping) function in this example. This means that $\\pi(\\theta'|\\theta) = \\pi(\\theta|\\theta')$.\n",
    "If the proposal function is not symmetric, we should adjust the acceptance ratio as follows:\n",
    "$$ H= \\frac{P(y | \\theta ') \\times P(\\theta ') \\times \\pi(\\theta|\\theta') }{P(y | \\theta) \\times P(\\theta) \\times \\pi(\\theta' |\\theta)} \\quad .$$\n",
    "\n",
    "- **Parameter transformation.** When parameter supports are bounded (e.g. finite interval), \n",
    "we often transform the parameters into quantities that are unbounded to make sampling easier. \n",
    "For example, with the transformed parameter space, \n",
    "we don't have to worry about having a proposal function defined on a bounded support. \n",
    "These transformations imply some more adjustments to the acceptance ratio that are not discussed here.\n",
    "\n",
    "- **Thinning.** The samples generated by some MCMC algorithms (e.g. Metropolis-Hastings) are often highly auto-correlated. This is due to the iterative way in which the samples are generated. To address this issue we often apply thinning after generating the samples. That is, we only retain every n-th sampled paramerer sets.\n",
    "\n",
    "- **Algorithm tuning.** This is probably the most challenging aspect of the Metropolis-Hastings sampler. This is about finding an adequate proposal (jumping) function that will ensure an exhaustive and efficient exploration of the parameter space. If transitions (or jumps) are too big, we will rarely accept the proposed parameters because they would be outside the high-density regions. If transitions are too small, we may not explore the parameter space comprehensively because we may always stay in the same regions. There is no pre-defined rule about how to define a \"good\" proposal function, but we often want to achieve an acceptance rate of about 10-40%.\n",
    "\n",
    "## How we actually approach calibration\n",
    "Having shown this manual implementation of the Metropolis algorithm,\n",
    "we should stresss that this is actually not how we typically calibrate models.\n",
    "There are multiple libraries that handle Bayesian sampling with MCMC algorithms already implemented.\n",
    "These libraries have a range of additional features, \n",
    "such as self-tuning and automatic parameter transformation, \n",
    "that can address several of the issues listed above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "7afc08b952f75bca94590012dd49682c815a0fa68720c270ce23d7ae27bf110a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
